I used 10 fold cross validation on the labeled data set for the 311 data.  Since I shuffled the data I got sligtly different values each time, to try and make it as acurate as possible.  The 3rd order polynomial had the lowest RMSE and highest R squared value most of the time, so that was what I chose for my model.  Using this 3rd order line, it predicted the values for the unlabeled data.  Hopefully this RMSE will be close to those that was predicted when doing the cross validation.  One problem with this unlabeled data in general(as studied in problem a) is that a lot of the data has very few incidents(which I'm not saying is a bad thing!) but it doesn't go with the general trend of the data.  Therefore, they polynomial lines of fit are pulled down slightly because of that, and therefore the predicted values are pulled down because they are based on that line, which might skew the values.  One way to improve this is to examine those places that have the low incident rate and see why and where they are.  Perhaps it would be best to group this data by region or borough, because each region probably follows more similar trends than every zipcode all together, and the polynoimial would therefore fit each set of data better.