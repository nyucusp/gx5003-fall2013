Report for Assigment 6
Christopher Jacoby
gx5003 - Principles of Urban Informatics

a. From plotting the data, with population vs the number of incidents,
it is clear that the data could be represented linearly. There are two observable features that could
interfere with this, however:
    1) as the poplation increases, the standard deviation also seems to increase
    2) There are a significant amount of near-zero values that do not correspond to the line produced.


b. I allow the cross-validations to choose the mode based on the results of the RSME and R^2s.
    I use the argmin of the RMSE's for each fold chose, and the argmax of the R^2s, and then
    take the mode of these across all of the fold runs. So far this has returned orders of 1 and 4
    for the conclusions, with it tending towards 1, but occasionally returning 4. There is a noticable
    decrease/increase in the RMSE/R^2 at order 4 for these models, and there are frequent occurences
    of 4s for individual sets of folds, but on most runs, the mode of the results still gives an order 1
    model for the results.

c. The RMSE for the whole dataset lines up nicely with the average RMSE from the model that was
    selected. (Over several runs, however, order 4 was selected serveral times; it still corresponded
    well with that, although in general there were more order 1's.). The standard deviations for the
    chosen order fit nicely around the RMSE for the whole dataset, and tend not to include it so well
    for the other orders. Order 5 is always way higher in RMSE than every other one.

d. During the process for part d, I spent a significant amount of time working with a dataset from the IRS
    containing zip-code related information based on tax returns. I was attempting to relate the Adjusted Gross Income
    to the number of incidents. Plotting a graph (d1) shows that there is a correlation here, and it is relatively
    close to that of the original data. (I attempted to do an individual comparison of a model generated by
    each of these and the population, but I was not successful).

Predictions for the test data:
Zip Code, Prediction
11789, -2560.0
11797, -1712.0
11801,  20352.0
11577,  672.0
10801,  21248.0
10307,  1952.0
10308,  11584.0
10314,  53760.0
11361,  12480.0
11364,  16768.0
11365,  22272.0
11372,  40064.0
11373, 64768.0
14450,  21504.0
14467, -1680.0
14614, -7384.0
11412,  17024.0
11416,  9792.0
11204,  48384.0
11419,  25984.0
11421,  20096.0
11426,  4480.0
11428,  5632.0
10921, -5272.0
11434,  34432.0
11948, -7454.0
11378,  17088.0
14009, -4160.0
11964, -6920.0
10941,  1728.0
10952,  19840.0
11042, 7890.0
10452,  46336.0
10454,  18752.0
10460,  33280.0
10461,  28288.0
10463,  40960.0
10465,  22272.0
14051,  5888.0
14052,  4320.0
10471,  8320.0
10473,  34048.0
10475,  21376.0
11561,  18752.0
11001,  11200.0
11003,  21760.0
11005, -6960.0
11520,  23168.0
13057,  2624.0
10502, -4304.0
10504, -2480.0
12553,  9472.0
11023, -1728.0
10512,  10304.0
10003,  32256.0
10005, -3104.0
11030,  4768.0
10009,  36096.0
10010,  14848.0
11040,  21248.0
10017,  3744.0
10530,  896.0
10019,  22784.0
11557, -2592.0
11558, -2208.0
11559, -2352.0
10024,  34688.0
10025,  60416.0
11565, -1968.0
10030,  11328.0
10031,  32512.0
10550,  18624.0
10553,  -864.0
11580,  20736.0
11581,  6848.0
10566,  8832.0
10570,  928.0
10580,  4192.0
10583,  19968.0
11096, -2240.0
11741,  11776.0
11103,  19712.0
10598,  12480.0
10603,  4096.0
14701,  21120.0
14710, -5704.0
14203, -7092.0
14209, -2528.0
14228,  6848.0
11590,  24832.0
11691,  35200.0
11692,  5152.0
11693,  384.0
11697, -5304.0
10162, -7048.0
11703,  3648.0
11714,  8192.0
11716,  -512.0
11206,  50944.0
11207,  59392.0
11720,  12864.0
11209,  41472.0
11212,  52992.0
10701,  37632.0
11221,  49152.0
11222,  18496.0
11229,  49920.0
13790,  5568.0
11233,  40320.0
11234,  55552.0
11239,  1440.0
11766,  896.0
11768,  7616.0
11772,  24192.0

e. I expect the RMSE on the unlabeled test dataset to be moderately high. The model that is produced by this, while it matches
    up visually with the slope and the general trend of the training data, is significantly offset because of the large amount of data
    with near zero incidents. Given more time, I think the best way to massage a better model would be to eliminate these outliers.
    This would cause the model to shift upwards, and therefore to represent the trend of the existing, and therefore hypothetically, other
    data points, better. I don't think that having too many other correlating variables would help the results as much as reducing the outliers.
    Alternatively, one could treat the data as two "clusters", with one being the one that is represented by the model, and the other
    by the near-zero data. If the points lie within the near-zero data, they would use a model that fits that data, and otherwise they
    would use the primary model.

