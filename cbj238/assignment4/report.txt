Handling Data Types:
	I have designed my database schema to have the types of the first data entry in each column. That is, if the data returns as an "int" in python, it is created as an int in sql, and the same with float. Anything else becomes a varchar(255).

	However, this assumption becomes difficult when reading them in, as there are some cases of zip codes where the zip codes seem to have alpha characters in them, like 123NN. I have decided to treat these as invalid zip codes, and to not include any entry into the database that has invalid data.

Dealing with duplicates and empty values:
	for boroughs.csv, in which there were a number of duplicate entries, I made a dictionary of the items in python during conversion to handle duplicates. Then, I made the zip code the primary key.

	for zipcodes.csv:
		* I threw out zip codes that were not valid numbers.
		* There are duplicate zip entries. I first checked to see if one of them was missing data.
			If so, I keep the one not missing data. Otherwise, I take the one with the larger area (arbitrarily)
			As it turned out, only two of them got updated, and most of them were missing data. Just threw those out.
		* made zipcodes (aka names) the primary key.

	for incidents...:
		* Addresses can be null, but zip codes shouldn't be, since we only reference things in this file by zips. Don't bother including those.
		Using an auto-increment primary key, since none of the entries here are unique.

		Another note about problems that I found in this file: a number of entries include single quotes as
			apostrophies. Had to use double quotes instead to catch them.

