K-means clustering approach

Overall, the K-means approach performed better than random selection (p<0.01) but was a poor predictor of letters. For instance, training over the whole data set of 16,000 points and then using the centroids to predict the letters of the remaining 4,000 points resulted in 13024 errors of 16000 predictions (random guessing would produce ~15386 errors for comparison).

The cross validation score on the 10K dataset was


K-nn approach

Overall, the K-nn approach was highly accurate.

Interestingly, the accuracy of K-nn seemed to go down as K increased. For example, on a small training set (1000 training points and 100 validation points) the number of errors made by K-nn was 30% when only the closest neighbor was considered, 37% when the closest 5 neighbors were considered, 45% when the closest 10 neighbors were considered, and 53% when the closest 50 neighbors were considered. This result appeared to hold when an even larger training and validation set were applied. Hence I used a relatively small value of K=3 for the more lengthy analyses.

Using the entire training data set of 16,000 data points to predict the letters of the 4,000 remaining points, the algorithm did quite well. Overall, only 183 errors were made of 4000 predictions (by contrast, the K-means clustering approach made 13024 errors of 16000 predictions). The F1 score for letter A using this approach was 0.993548387097
