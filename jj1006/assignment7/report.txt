K-means clustering approach

Overall, the K-means approach performed better than random selection (p<0.01) but was a poor predictor of letters. For instance, training over the whole data set of 16,000 points and then using the centroids to predict the letters of the remaining 4,000 points resulted in 3240 errors of 4000 predictions (random guessing would produce ~3846 errors for comparison).

The cross validation score on the 16K dataset was as follows:
CV #0. Made 1271 errors of 1600 predictions.
CV #1. Made 1287 errors of 1600 predictions.
CV #2. Made 1293 errors of 1600 predictions.
CV #3. Made 1286 errors of 1600 predictions.
CV #4. Made 1271 errors of 1600 predictions.
CV #5. Made 1260 errors of 1600 predictions.
CV #6. Made 1270 errors of 1600 predictions.
CV #7. Made 1292 errors of 1600 predictions.
CV #8. Made 1254 errors of 1600 predictions.
CV #9. Made 1291 errors of 1600 predictions.
Mean % error = 0.7984375


K-nn approach

Overall, the K-nn approach was more accurate than the Kmeans clustering approach.

Interestingly, the accuracy of K-nn seemed to go down as K increased. For example, on a small training set (1000 training points and 100 validation points) the number of errors made by K-nn was 30% when only the closest neighbor was considered, 37% when the closest 5 neighbors were considered, 45% when the closest 10 neighbors were considered, and 53% when the closest 50 neighbors were considered. This result appeared to hold when an even larger training and validation set were applied. Hence I used a relatively small value of K=3 for the more lengthy analyses.

The cross validation score took too long to compute for the full 16K dataset, so I used a smaller 5K dataset instead:
CV #0. Made 209 errors of 500 predictions.
CV #1. Made 200 errors of 500 predictions.
CV #2. Made 221 errors of 500 predictions.
CV #3. Made 190 errors of 500 predictions.
CV #4. Made 206 errors of 500 predictions.
CV #5. Made 217 errors of 500 predictions.
CV #6. Made 208 errors of 500 predictions.
CV #7. Made 211 errors of 500 predictions.
CV #8. Made 207 errors of 500 predictions.
CV #9. Made 198 errors of 500 predictions.
Mean % error = 0.4134

Using the entire training data set of 16,000 data points to predict the letters of the 4,000 remaining points, the algorithm did quite well. Overall, only 183 errors were made of 4000 predictions (by contrast, the K-means clustering approach made 3240 errors of 4000 predictions). The F1 score for letter A using this approach was 0.99354838709
